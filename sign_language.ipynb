{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____________________ Sign Language Detection ____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Too many arguments.\n",
      "\n",
      "usage: git clone [<options>] [--] <repo> [<dir>]\n",
      "\n",
      "    -v, --verbose         be more verbose\n",
      "    -q, --quiet           be more quiet\n",
      "    --progress            force progress reporting\n",
      "    --reject-shallow      don't clone shallow repository\n",
      "    -n, --no-checkout     don't create a checkout\n",
      "    --bare                create a bare repository\n",
      "    --mirror              create a mirror repository (implies bare)\n",
      "    -l, --local           to clone from a local repository\n",
      "    --no-hardlinks        don't use local hardlinks, always copy\n",
      "    -s, --shared          setup as shared repository\n",
      "    --recurse-submodules[=<pathspec>]\n",
      "                          initialize submodules in the clone\n",
      "    --recursive ...       alias of --recurse-submodules\n",
      "    -j, --jobs <n>        number of submodules cloned in parallel\n",
      "    --template <template-directory>\n",
      "                          directory from which templates will be used\n",
      "    --reference <repo>    reference repository\n",
      "    --reference-if-able <repo>\n",
      "                          reference repository\n",
      "    --dissociate          use --reference only while cloning\n",
      "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
      "    -b, --branch <branch>\n",
      "                          checkout <branch> instead of the remote's HEAD\n",
      "    -u, --upload-pack <path>\n",
      "                          path to git-upload-pack on the remote\n",
      "    --depth <depth>       create a shallow clone of that depth\n",
      "    --shallow-since <time>\n",
      "                          create a shallow clone since a specific time\n",
      "    --shallow-exclude <revision>\n",
      "                          deepen history of shallow clone, excluding rev\n",
      "    --single-branch       clone only one branch, HEAD or --branch\n",
      "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
      "    --shallow-submodules  any cloned submodules will be shallow\n",
      "    --separate-git-dir <gitdir>\n",
      "                          separate git dir from working tree\n",
      "    -c, --config <key=value>\n",
      "                          set config inside the new repository\n",
      "    --server-option <server-specific>\n",
      "                          option to transmit\n",
      "    -4, --ipv4            use IPv4 addresses only\n",
      "    -6, --ipv6            use IPv6 addresses only\n",
      "    --filter <args>       object filtering\n",
      "    --also-filter-submodules\n",
      "                          apply partial clone filters to submodules\n",
      "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
      "    --sparse              initialize sparse-checkout file to include only files at root\n",
      "    --bundle-uri <uri>    a URI for downloading bundles before fetching from origin remote\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\xxram\\anaconda3\\lib\\site-packages (1.13.0+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\xxram\\anaconda3\\lib\\site-packages (0.14.0+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\xxram\\anaconda3\\lib\\site-packages (0.13.0+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\xxram\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -r requirements.txt  # install\n",
    "\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import threading\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Roboflow for manual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"UxYM4k1Bj7JLXRh9oXIu\")\n",
    "# project = rf.workspace(\"handrecog\").project(\"hand_images\")\n",
    "# dataset = project.version(1).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")\n",
    "\n",
    "# rf = Roboflow(api_key=\"Sbc4IW9leBI9v22H2p11\")\n",
    "# project = rf.workspace(\"signlanguage-7gl6w\").project(\"letter_images\")\n",
    "# dataset = project.version(2).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\n",
    "\n",
    "!python train.py --img 416 --batch 32 --epoch 150 --data sign_language_dataset.yaml --weights yolov5m.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Start Time: 10:06\n",
    "#   End Time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp44/weights/best.pt'], source=letter_images-2/valid/images, data=data\\coco128.yaml, imgsz=[416, 416], conf_thres=0.3, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v6.2-267-gbe348cc Python-3.8.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20945877 parameters, 0 gradients, 48.2 GFLOPs\n",
      "image 1/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\a_003_jpg.rf.72c0db7d801527c0a5d9fc300e394b99.jpg: 416x416 1 a, 13.0ms\n",
      "image 2/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\b_002_jpg.rf.3863714fa03996a66f4ec630b3919cbc.jpg: 416x416 1 b, 15.0ms\n",
      "image 3/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\b_003_jpg.rf.611ea658aa986711531996fec20ba860.jpg: 416x416 1 w, 14.0ms\n",
      "image 4/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\b_003_jpg.rf.78a06aed1e590e8744058afd8c73caf0.jpg: 416x416 1 w, 14.0ms\n",
      "image 5/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\c_003_jpg.rf.36e871a3d9ac843f8c6e9bccbc3b9218.jpg: 416x416 1 p, 14.0ms\n",
      "image 6/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\c_003_jpg.rf.6a86805d6f0fbb3f0aecffaf1e32ec9f.jpg: 416x416 1 p, 15.0ms\n",
      "image 7/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\d_002_jpg.rf.ce4afe4d29b18e93c665f48b912f0e35.jpg: 416x416 1 d, 13.0ms\n",
      "image 8/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\e_002_jpg.rf.2577537f71da65f70722b7d7dcc79b6f.jpg: 416x416 1 e, 15.0ms\n",
      "image 9/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\f_001_jpg.rf.b7446e565713dc873741721a02f06eff.jpg: 416x416 1 f, 14.0ms\n",
      "image 10/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\f_002_jpg.rf.239c312ffff9615ebfa47a7667864a86.jpg: 416x416 1 f, 15.0ms\n",
      "image 11/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\f_003_jpg.rf.291082b760701b5e551570ea7b119560.jpg: 416x416 1 w, 15.0ms\n",
      "image 12/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\f_003_jpg.rf.729ab2e183d1f3a7b904efb2f56eac7d.jpg: 416x416 1 w, 15.0ms\n",
      "image 13/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\g_001_jpg.rf.7e86499dbb99d07e99fdc00ae26d3576.jpg: 416x416 1 g, 14.0ms\n",
      "image 14/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\g_002_jpg.rf.062fe774ad82dd00cc4656d2e0beb2cc.jpg: 416x416 1 g, 14.0ms\n",
      "image 15/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\g_002_jpg.rf.be2e8c244d65d9bf38a35ea7147ff48f.jpg: 416x416 1 g, 14.0ms\n",
      "image 16/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\i_003_jpg.rf.2aad8d1dc33ad07dbc987b423f206e37.jpg: 416x416 1 i, 13.0ms\n",
      "image 17/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\k_002_jpg.rf.0d1bf0e80eba1fbc92eb2de6bbbd2c7a.jpg: 416x416 1 k, 14.0ms\n",
      "image 18/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\l_002_jpg.rf.ba7516422bb08076d29b4d6043731586.jpg: 416x416 1 l, 15.0ms\n",
      "image 19/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\m_003_jpg.rf.32d2bae007e786fde620dc803e29e839.jpg: 416x416 1 m, 13.0ms\n",
      "image 20/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\n_001_jpg.rf.bb7cf8aa38c928dce5c2ae640ef39a85.jpg: 416x416 1 n, 15.0ms\n",
      "image 21/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\n_003_jpg.rf.2b80a9168d87f2c1ae44f3985d7cf642.jpg: 416x416 1 n, 13.0ms\n",
      "image 22/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\o_001_jpg.rf.f907c9ab4df966a7def45ec188f087a3.jpg: 416x416 1 o, 15.0ms\n",
      "image 23/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\p_002_jpg.rf.b47104fdc4de0e3a941d7aa0bb594c6d.jpg: 416x416 1 p, 14.0ms\n",
      "image 24/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\p_003_jpg.rf.6585167eb3a323bba31a97545a1dc6f6.jpg: 416x416 1 p, 15.0ms\n",
      "image 25/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\r_001_jpg.rf.9d86dc7c8a062cb6890458cfb1e22c9b.jpg: 416x416 1 r, 14.0ms\n",
      "image 26/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\r_002_jpg.rf.79f7c4988432474c67743c2ea54ef8fb.jpg: 416x416 1 r, 14.0ms\n",
      "image 27/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\s_003_jpg.rf.a0539cc47e2d458ea0418ca0931bc739.jpg: 416x416 1 s, 15.0ms\n",
      "image 28/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\s_003_jpg.rf.fd622495256e60bbad50ad58f247b7da.jpg: 416x416 1 s, 15.0ms\n",
      "image 29/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\t_001_jpg.rf.26225153ac2e214a03e6f34135c4ad23.jpg: 416x416 1 u, 15.0ms\n",
      "image 30/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\t_001_jpg.rf.98a245fc9be9ab04da6fa78f11d7f3ce.jpg: 416x416 1 u, 13.0ms\n",
      "image 31/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\u_001_jpg.rf.23a09f5d04f73cc688a8e7106861ef92.jpg: 416x416 1 u, 14.0ms\n",
      "image 32/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\u_002_jpg.rf.c077ddcf2137521600a94e35a01a66f3.jpg: 416x416 1 u, 14.0ms\n",
      "image 33/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\v_001_jpg.rf.fe8e6c4c6eeac43732e9e9efd595d230.jpg: 416x416 1 v, 14.0ms\n",
      "image 34/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\w_003_jpg.rf.20f94bf51ecb2226040aab0d3fd5112c.jpg: 416x416 1 w, 15.0ms\n",
      "image 35/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\w_003_jpg.rf.a40d0366fa7c0452d2a39d739e89ecff.jpg: 416x416 1 w, 14.0ms\n",
      "image 36/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\x_001_jpg.rf.1e42efaf5748803d559dd4dbc13aea7d.jpg: 416x416 1 x, 13.0ms\n",
      "image 37/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\x_002_jpg.rf.71d4007481ec76f427d35dbb5d5768eb.jpg: 416x416 1 x, 14.0ms\n",
      "image 38/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\x_003_jpg.rf.039feb16c35f59a102a2baf6c4c8d49f.jpg: 416x416 (no detections), 13.0ms\n",
      "image 39/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\x_003_jpg.rf.33cdbfe5c80b2df931683fdd6457a35c.jpg: 416x416 (no detections), 14.0ms\n",
      "image 40/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\y_001_jpg.rf.9ee467848d8bfdabd4119a07f196c182.jpg: 416x416 1 y, 14.0ms\n",
      "image 41/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\y_002_jpg.rf.30455be25317d65c486008762cfa85e9.jpg: 416x416 1 y, 15.0ms\n",
      "image 42/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\y_003_jpg.rf.0ca536f4684ca830bfdfd3bca243aa6c.jpg: 416x416 1 y, 14.0ms\n",
      "image 43/43 C:\\Users\\xxram\\OneDrive\\Desktop\\Project\\gesture_recognition\\yolov5\\Letter_images-2\\valid\\images\\z_002_jpg.rf.53abe946dbdbaae9a37ba171a57fadec.jpg: 416x416 1 z, 16.0ms\n",
      "Speed: 0.2ms pre-process, 14.2ms inference, 1.8ms NMS per image at shape (1, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns\\detect\\exp21\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights runs/train/exp44/weights/best.pt --img 416 --conf 0.3 --source letter_images-2/valid/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\xxram/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-1-19 Python-3.8.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20945877 parameters, 0 gradients, 48.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='../yolov5/runs/train/exp44/weights/best.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__\n",
    "\n",
    "# Run the below commands in terminal if the there is an error:\n",
    "#\n",
    "# pip uninstall opencv-python\n",
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns on the camera after the user presses 'Start'. Freezes the frame when the user presses 'Stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_button_clicked(self):\n",
    "    global letter\n",
    "    letter_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"i\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "    letter = random.choice(letter_list)\n",
    "\n",
    "\n",
    "def start_button_clicked(videoloop_stop):\n",
    "    threading.Thread(target=videoLoop, args=(videoloop_stop,)).start()\n",
    "\n",
    "\n",
    "def stop_button_clicked(videoloop_stop):\n",
    "    videoloop_stop[0] = True\n",
    "\n",
    "\n",
    "def videoLoop(mirror=False):\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 600)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        results = model(frame)\n",
    "\n",
    "        for i, row in results.pandas().xyxy[0].iterrows():\n",
    "            name = (row['name'])\n",
    "            conf = (row['confidence'])\n",
    "            if conf >= 0.5:\n",
    "                name_label = tk.Label(root, text=name, bg=\"green\", font=(\"\", 20))\n",
    "                name_label.place(x=50, y=600, width=400, height=150)\n",
    "                # print(name, \" with confidence: {:.2f}\".format(conf))\n",
    "            elif conf >= 0.3 and conf < 0.5:\n",
    "                almost_label = tk.Label(root, text = \"Nearly There!\", bg = \"yellow\", font=(\"\", 20))\n",
    "                almost_label.place(x=50, y=600, width=400, height=150)\n",
    "            elif conf < 0.3:\n",
    "                low_label = tk.Label(root, text = \"Needs Improvement!\", bg = \"red\", font=(\"\", 20))\n",
    "                low_label.place(x=50, y=600, width=400, height=150)\n",
    "            \n",
    "        if mirror is True:\n",
    "            frame = frame[:, ::-1]\n",
    "            \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = ImageTk.PhotoImage(image)\n",
    "        panel = tk.Label(image=image)\n",
    "        panel.image = image\n",
    "        panel.place(x=50, y=120)\n",
    "\n",
    "        if videoloop_stop[0]:\n",
    "            videoloop_stop[0] = False\n",
    "            panel.destroy()\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "       \n",
    "\n",
    "videoloop_stop = [False]\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"BSL Learning Tool\")\n",
    "root.geometry(\"1920x1080+0+0\")\n",
    "\n",
    "letter_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"i\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", ]\n",
    "letter = random.choice(letter_list)\n",
    "\n",
    "random_button = tk.Button(root, text=\"New letter\", bg=\"#fff\", font=(\"\", 30),command= random_button_clicked)\n",
    "random_button.place(x=1000, y=100, width=400, height=150)\n",
    "\n",
    "start_button = tk.Button(root, text=\"start\", bg=\"#fff\", font=(\"\", 50),command=lambda: start_button_clicked(videoloop_stop))\n",
    "start_button.place(x=1000, y=500, width=400, height=150)\n",
    "\n",
    "stop_button = tk.Button(root, text=\"stop\", bg=\"#fff\", font=(\"\", 50),command=lambda: stop_button_clicked(videoloop_stop))\n",
    "stop_button.place(x=1000, y=650, width=400, height=150)\n",
    "\n",
    "prompt_label = tk.Label(root, text=(\"Sign\", letter), font=(\"\", 30))\n",
    "prompt_label.place(x=50, y=50)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a322872bde48165c8b38fc07aafe23838cb691b5065cdeed8882e01b10b50872"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
